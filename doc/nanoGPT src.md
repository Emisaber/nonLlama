
## `model.py`


```python
import math
import inspect
from dataclasses import dataclass
```
- dataclassä½œä¸ºdecoratorï¼Œç”¨äºè‡ªåŠ¨ä¸ºç±»æ·»åŠ `__init__()` `__repr__()`, `__eq__()`ï¼Œ`__ne__()` ç­‰æ–¹æ³•   
- `inspect` ç”¨äºè®¿é—®ä¼˜åŒ–å™¨çš„å‚æ•°

#### LayerNorm 
ä¸ºäº†èƒ½å°†biasè®¾ä¸ºNone   
æœ‰æ—¶é—´çš„è¯è¡¥ä¸€ä¸‹LayerNormï¼Ÿ   
```python
class LayerNorm(nn.Module):
    """ LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False """

    def __init__(self, ndim, bias):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None

    def forward(self, input):
        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)
```
> æä¾›äº†ä¸€ä¸ªNoneçš„æ¥å£ï¼Œåº”è¯¥æ˜¯ä¸ºäº†æ–¹ä¾¿åé¢çš„å‚æ•°è®¡ç®—


#### Attention block  

```python
class CausalSelfAttention(nn.Module):
```


**åˆå§‹åŒ–**  
```python
def __init__(self, config):
	super().__init__()
	assert config.n_embd % config.n_head == 0 # å¦‚æœä¸èƒ½æ•´é™¤ä¸­æ–­
	# key, query, value projections for all heads, but in a batch
	self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
	# output projection
	self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
	# regularization
	self.attn_dropout = nn.Dropout(config.dropout)
	self.resid_dropout = nn.Dropout(config.dropout)
	self.n_head = config.n_head
	self.n_embd = config.n_embd
	self.dropout = config.dropout
	# flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0
	self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
	if not self.flash:
		print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")
		# causal mask to ensure that attention is only applied to the left in the input sequence
		self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))
```
- åˆå§‹åŒ–QKVæƒé‡çŸ©é˜µ(åœ¨è¿™é‡Œå°†å®ƒä»¬ä½œä¸ºä¸€ä¸ªbatchå¾—åˆ°)
- æŠ•å½±çŸ©é˜µç”¨äºæ®‹å·®è¿æ¥å‰è¿›è¡ŒæŠ•å½±ï¼Œå®šä¹‰æ³¨æ„åŠ›çš„dropoutï¼Œæ®‹å·®è¿æ¥çš„dropoutï¼Œå®šä¹‰æ³¨æ„åŠ›å¤´æ•°ï¼Œembeddingé•¿åº¦ï¼Œdropoutæ¦‚ç‡  
- é€šè¿‡`hasattr` åˆ¤æ–­å½“å‰torchæ˜¯å¦æœ‰flash attentionï¼Œæ²¡æœ‰çš„è¯ä¸åŸç†è§†é¢‘å®ç°ä¸€è‡´
	- è¿™é‡Œçš„`view`åº”è¯¥ä¸åŠ ä¹Ÿè¡Œ

**å‰ä¼ **  
```python
def forward(self, x):
	B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)

	# calculate query, key, values for all heads in batch and move head forward to be the batch dim
	q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)
	k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
	q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
	v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

	# causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
	if self.flash:
		# efficient attention using Flash Attention CUDA kernels
		y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
	else:
		# manual implementation of attention
		att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
		att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
		att = F.softmax(att, dim=-1)
		att = self.attn_dropout(att)
		y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
	y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

	# output projection
	y = self.resid_dropout(self.c_proj(y))
	return y
```
- å°†æƒé‡çŸ©é˜µåˆ‡æˆä¸‰ä»½
- ä¸åŸç†è§†é¢‘ä¸åŒçš„åœ¨äºåŒºåˆ†äº†ä¸åŒæ³¨æ„åŠ›å¤´ `B, nh, T, hs`
- flash attention æˆ–è€… slow attention è¾“å‡ºä¸º `B, nh, T, T`
	- æ³¨æ„åŠ›çŸ©é˜µçš„æœ€åä¸€ç»´ä¸ºæ—¶é—´
- `y = y.transpose(1,2).contiguous().view(B, T, C)`  å°†ä¸åŒæ³¨æ„åŠ›å¤´çš„è¾“å‡ºæ‹¼èµ·æ¥
- æœ€åè¿›è¡ŒæŠ•å½±å¹¶dropout


#### MLP

```python
class MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
        self.gelu    = nn.GELU()
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x
```
- ä½¿ç”¨ä¸transformerä¸€è‡´çš„4å€ä¸­é—´çŠ¶æ€
- å°†åŸç†è§†é¢‘ä¸­çš„ReLUæ¢æˆGELU

#### Block

Transformer block  
```python
class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x
```

#### GPT

```python
@dataclass
class GPTConfig:
    block_size: int = 1024
    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    dropout: float = 0.0
    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
```
- dataclassè‡ªåŠ¨æ·»åŠ äº†åŸºæœ¬çš„ç±»æ–¹æ³•ï¼Œç”¨äºå¾—åˆ°ç»Ÿä¸€çš„GPT config


```python
class GPT(nn.Module):
```

```python
def __init__(self, config):
	super().__init__()
	assert config.vocab_size is not None
	assert config.block_size is not None
	self.config = config

	self.transformer = nn.ModuleDict(dict(
		wte = nn.Embedding(config.vocab_size, config.n_embd),
		wpe = nn.Embedding(config.block_size, config.n_embd),
		drop = nn.Dropout(config.dropout),
		h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
		ln_f = LayerNorm(config.n_embd, bias=config.bias),
	))
	self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
	# with weight tying when using torch.compile() some warnings get generated:
	# "UserWarning: functional_call was passed multiple values for tied weights.
	# This behavior is deprecated and will be an error in future versions"
	# not 100% sure what this is, so far seems to be harmless. TODO investigate
	self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

	# init all weights
	self.apply(self._init_weights)
	# apply special scaled init to the residual projections, per GPT-2 paper
	for pn, p in self.named_parameters():
		if pn.endswith('c_proj.weight'):
			torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))

	# report number of parameters
	print("number of parameters: %.2fM" % (self.get_num_params()/1e6,))
```
- Weight Tying
	- embedding å’Œ softmaxå±‚æƒé‡å…±äº«ï¼Œèƒ½ä¼˜åŒ–æ¨¡å‹è¡¨ç°ï¼ˆAttention is all you need é‡‡ç”¨äº†è¿™ä¸ªæ–¹æ³•ï¼‰
	- ä»£ç ä¸­å®ç° `self.transformer.wte.weight = self.lm_head.weight`ï¼Œä½†æ˜¯æœ‰warning
	- [Why Weight Tying](https://emisaber.github.io/White_Box/Notes/Why-Weight-Tying)
- `self.apply(self._init_weights)`  
	- å¯¹æ¨¡å‹è¿›è¡Œç‰¹æ®Šçš„æƒé‡åˆå§‹åŒ– 
	- maybe an empirical method

**è·å¾—æ€»å‚æ•°é‡**   
```python
def get_num_params(self, non_embedding=True):
	"""
	Return the number of parameters in the model.
	For non-embedding count (default), the position embeddings get subtracted.
	The token embeddings would too, except due to the parameter sharing these
	params are actually used as weights in the final layer, so we include them.
	"""
	n_params = sum(p.numel() for p in self.parameters())
	if non_embedding:
		n_params -= self.transformer.wpe.weight.numel()
	return n_params
```
- è®¡ç®—å‚æ•°é‡ï¼Œ`numel()` è¿”å›tensor elementçš„ä¸ªæ•°
- é»˜è®¤ä¸åŠ ç®—embeddingçš„å‚æ•°é‡
	- Transformerä¸­åº”æ˜¯æ²¡æœ‰position embedding å‚æ•°çš„ï¼Œä¸€å¼€å§‹çš„embeddingçš„å‚æ•°ä¹Ÿåº”è¯¥ä¸è¢«åŒ…å«ï¼Œä½†æ˜¯å®ƒå’ŒæŠ•å½±å±‚å…±äº«ï¼Œæ‰€ä»¥å®é™…ä¸Šä¹Ÿè®¡ç®—åœ¨å†…

**åˆå§‹åŒ–æƒé‡**(**æ˜“äºæ”¶æ•›**)
```python
def _init_weights(self, module):
	if isinstance(module, nn.Linear):
		torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
		if module.bias is not None:
			torch.nn.init.zeros_(module.bias)
	elif isinstance(module, nn.Embedding):
		torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
```
- ğŸ¤” è¿™ä¸ª`std=0.02`åº”æ˜¯å®éªŒç»“è®º


```python
def forward(self, idx, targets=None):
	device = idx.device
	b, t = idx.size()
	assert t <= self.config.block_size, f"Cannot forward sequence of length {t}, block size is only {self.config.block_size}"
	pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)

	# forward the GPT model itself
	tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
	pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)
	x = self.transformer.drop(tok_emb + pos_emb)
	for block in self.transformer.h:
		x = block(x)
	x = self.transformer.ln_f(x)

	if targets is not None:
		# if we are given some desired targets also calculate the loss
		logits = self.lm_head(x)
		loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
	else:
		# inference-time mini-optimization: only forward the lm_head on the very last position
		logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
		loss = None

	return logits, loss
```
- å‰ä¼ è¿‡ç¨‹
	- token embedding + pos embedding
	- dropout
	- each block in transformer
	- layernorm at the end of transformer
	- return loss if traning else logits only

**è£å‰ªä¸Šä¸‹æ–‡**  
```python
def crop_block_size(self, block_size):
	# model surgery to decrease the block size if necessary
	# e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)
	# but want to use a smaller block size for some smaller, simpler model
	assert block_size <= self.config.block_size
	self.config.block_size = block_size
	self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])
	for block in self.transformer.h:
		if hasattr(block.attn, 'bias'):
			block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]
```
- å¦‚æœéœ€è¦æ›´å°çš„block sizeï¼Œå¯¹æ¨¡å‹çš„å‚æ•°è¿›è¡Œè£å‰ª

**ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½**  
```python
@classmethod
def from_pretrained(cls, model_type, override_args=None):
	assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}
	override_args = override_args or {} # default to empty dict
	# only dropout can be overridden see more notes below
	assert all(k == 'dropout' for k in override_args)
	from transformers import GPT2LMHeadModel
	print("loading weights from pretrained gpt: %s" % model_type)

	# n_layer, n_head and n_embd are determined from model_type
	config_args = {
		'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params
		'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params
		'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params
		'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params
	}[model_type]
	print("forcing vocab_size=50257, block_size=1024, bias=True")
	config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints
	config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints
	config_args['bias'] = True # always True for GPT model checkpoints
	# we can override the dropout rate, if desired
	if 'dropout' in override_args:
		print(f"overriding dropout rate to {override_args['dropout']}")
		config_args['dropout'] = override_args['dropout']
	# create a from-scratch initialized minGPT model
	config = GPTConfig(**config_args)
	model = GPT(config)
	sd = model.state_dict()
	sd_keys = sd.keys()
	sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param

	# init a huggingface/transformers model
	model_hf = GPT2LMHeadModel.from_pretrained(model_type)
	sd_hf = model_hf.state_dict()

	# copy while ensuring all of the parameters are aligned and match in names and shapes
	sd_keys_hf = sd_hf.keys()
	sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer
	sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)
	transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']
	# basically the openai checkpoints use a "Conv1D" module, but we only want to use a vanilla Linear
	# this means that we have to transpose these weights when we import them
	assert len(sd_keys_hf) == len(sd_keys), f"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}"
	for k in sd_keys_hf:
		if any(k.endswith(w) for w in transposed):
			# special treatment for the Conv1D weights we need to transpose
			assert sd_hf[k].shape[::-1] == sd[k].shape
			with torch.no_grad():
				sd[k].copy_(sd_hf[k].t())
		else:
			# vanilla copy over the other parameters
			assert sd_hf[k].shape == sd[k].shape
			with torch.no_grad():
				sd[k].copy_(sd_hf[k])

	return model
```
- æŠŠ`from transformers import GPT2LMHeadModel` å†™åœ¨å‡½æ•°å†…ï¼Œå¯èƒ½æ˜¯`from_pretrained`ä¸ä¸€å®šä¼šä½¿ç”¨ï¼Œè¿™æ ·æ¯”è¾ƒèŠ‚çº¦å†…å­˜å’Œé«˜æ•ˆ
- é€šè¿‡list comprehension `sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]` ä¸¢å¼ƒ buffer
- GPT openaiçš„å®ç°ä½¿ç”¨`Conv1D` è¿›è¡ŒæŠ•å½± `c_attn`æ˜¯æ³¨æ„åŠ›çš„æƒé‡ï¼Œ`c_proj`æ˜¯æ³¨æ„åŠ›ä¹‹åçš„embedding æŠ•å½±æƒé‡ï¼Œ`mlp.c_fc` `mlp.c_proj` æ˜¯MLPçš„ä¸¤ä¸ªçº¿æ€§å±‚
- `copy_()` inplace copyï¼Œå°†é¢„è®­ç»ƒæƒé‡ä¸­å·ç§¯çš„éƒ¨åˆ†è½¬åŒ–ä¸ºçº¿æ€§å±‚æƒé‡

**ä¼˜åŒ–å™¨è®¾ç½®**  
```python
def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):
	# start with all of the candidate parameters
	param_dict = {pn: p for pn, p in self.named_parameters()}
	# filter out those that do not require grad
	param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}
	# create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.
	# i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.
	decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]
	nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]
	optim_groups = [
		{'params': decay_params, 'weight_decay': weight_decay},
		{'params': nodecay_params, 'weight_decay': 0.0}
	]
	num_decay_params = sum(p.numel() for p in decay_params)
	num_nodecay_params = sum(p.numel() for p in nodecay_params)
	print(f"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters")
	print(f"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters")
	# Create AdamW optimizer and use the fused version if it is available
	fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters
	use_fused = fused_available and device_type == 'cuda'
	extra_args = dict(fused=True) if use_fused else dict()
	optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)
	print(f"using fused AdamW: {use_fused}")

	return optimizer
```
- å¯¹parametersåˆ†ç»„ï¼Œåªå¯¹æƒé‡å‚æ•°è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä¸å¯¹åç½®é¡¹è¿›è¡Œæ­£åˆ™åŒ–
	- åç½®é¡¹å¹¶ä¸ä¸»è¦å‚ä¸æ¨¡å‹è¾“å‡ºè®¡ç®—ï¼Œåªå¯¹ç»“æœè¿›è¡Œå¹³ç§»/ç¼©æ”¾ï¼Œåç½®é¡¹çš„å€¼é€šå¸¸æ¯”è¾ƒå°ï¼Œå¯¹åç½®é¡¹è¿›è¡Œæ­£åˆ™åŒ–æ•ˆæœä¸æ˜æ˜¾ï¼Œå¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆ(å½±å“äº†çµæ´»æ€§)
	- è¿™ä¸ªæ“ä½œåº”è¯¥å¯ä»¥å½“æˆæ¨¡æ¿
- `fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters`
	- `inspect.signature` å¯ä»¥æŸ¥çœ‹`callable`(`Adamw` here)çš„å‚æ•°
	- `fused`æ˜¯å¯¹ä¼˜åŒ–å™¨çš„æ€§èƒ½çš„ä¸€ç§ä¼˜åŒ–
	- è¿™ä¸ªä¹Ÿæ˜¯æ¨¡æ¿

**ä¼°è®¡MFU**   
```python
def estimate_mfu(self, fwdbwd_per_iter, dt):
	""" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS """
	# first estimate the number of flops we do per iteration.
	# see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311
	N = self.get_num_params()
	cfg = self.config
	L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size
	flops_per_token = 6*N + 12*L*H*Q*T
	flops_per_fwdbwd = flops_per_token * T
	flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter
	# express our flops throughput as ratio of A100 bfloat16 peak flops
	flops_achieved = flops_per_iter * (1.0/dt) # per second
	flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS
	mfu = flops_achieved / flops_promised
	return mfu
```
- MFU: model flops utilization
- FLOPsè®¡ç®—(æ¥è‡ªPaLM) $R = \frac{P}{6N+12LHQT}$ å…¶ä¸­ $6N$ æ¥è‡ªparameterï¼Œ$12LHQT$ æ¥è‡ªattentionçš„ä¸¤æ¬¡çŸ©é˜µè¿ç®—?ï¼Œåœ¨è¿™é‡Œè®¡ç®—åè¿‡æ¥äº†
- `dt`æ˜¯è¿­ä»£ä¸€æ¬¡çš„æ—¶é—´

**ç”Ÿæˆ**    
```python
@torch.no_grad()
def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
	"""
	Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete
	the sequence max_new_tokens times, feeding the predictions back into the model each time.
	Most likely you'll want to make sure to be in model.eval() mode of operation for this.
	"""
	for _ in range(max_new_tokens):
		# if the sequence context is growing too long we must crop it at block_size
		idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]
		# forward the model to get the logits for the index in the sequence
		logits, _ = self(idx_cond)
		# pluck the logits at the final step and scale by desired temperature
		logits = logits[:, -1, :] / temperature
		# optionally crop the logits to only the top k options
		if top_k is not None:
			v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
			logits[logits < v[:, [-1]]] = -float('Inf')
		# apply softmax to convert logits to (normalized) probabilities
		probs = F.softmax(logits, dim=-1)
		# sample from the distribution
		idx_next = torch.multinomial(probs, num_samples=1)
		# append sampled index to the running sequence and continue
		idx = torch.cat((idx, idx_next), dim=1)

	return idx
```
- é™åˆ¶è¾“å…¥çš„åºåˆ—é•¿åº¦(å¯¹è¿‡é•¿çš„æˆªæ–­)
- ä½¿ç”¨`topk`è¿›è¡Œgenerate



## `train.py`

training script   

#### hyperparameter 

```python
out_dir = 'out'
eval_interval = 2000
log_interval = 1
eval_iters = 200
eval_only = False # if True, script exits right after the first eval
always_save_checkpoint = True # if True, always save a checkpoint after each eval
init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'
# wandb logging
wandb_log = False # disabled by default
wandb_project = 'owt'
wandb_run_name = 'gpt2' # 'run' + str(time.time())
# data
dataset = 'openwebtext'
gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes
batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size
block_size = 1024
# model
n_layer = 12
n_head = 12
n_embd = 768
dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+
bias = False # do we use bias inside LayerNorm and Linear layers?
# adamw optimizer
learning_rate = 6e-4 # max learning rate
max_iters = 600000 # total number of training iterations
weight_decay = 1e-1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0
# learning rate decay settings
decay_lr = True # whether to decay the learning rate
warmup_iters = 2000 # how many steps to warm up for
lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
# DDP settings
backend = 'nccl' # 'nccl', 'gloo', etc.
# system
device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
compile = True # use PyTorch 2.0 to compile the model to be faster
```
- DDP:  DistributedDataParallel   
- `wandb`: æ˜¯ä¸€ä¸ªAI developer platformï¼Œæä¾›ä¸€ç³»åˆ—å·¥å…·ä¾¿åˆ©æ¨¡å‹è®­ç»ƒå’Œå¯è§†åŒ–   
- `gradient_accumulation_steps`: ç”¨äºåœ¨å°å†…å­˜çš„æƒ…å†µä¸‹æ¨¡æ‹Ÿå¤§batch sizeï¼Œæ¯ä¸€ä¸ªminibatchçš„gradientä¼šç´¯ç§¯ç›´åˆ°åˆ°è¾¾æŒ‡å®šçš„æ­¥æ•°   (å¦‚æœä½¿ç”¨gradient accumulation stepsï¼Œbatch sizeåº”è¯¥é€‚å½“å‡å°)    
- `weight_decay`:  weight_decayä»£è¡¨L2æ­£åˆ™åŒ–çš„èƒ½åŠ›
- `beta1, beta2`:  æ˜¯AdamWä¸­çš„ä¸¤ä¸ªå‚æ•°
- `decay_lr`, `warmup_iters`, `lr_decay_iters`, `min_lr` ç”¨äºè°ƒæ•´å­¦ä¹ ç‡  
	- Chinchillaæ˜¯è°·æ­Œscaling lawè®ºæ–‡çš„æ¨¡å‹åå­—ï¼Œä¸çŸ¥é“è¿™é‡ŒæŒ‡çš„æ˜¯ä»€ä¹ˆ  
- backend æ˜¯DDPçš„è®¾ç½®  
- `torch.compile`  æ˜¯pytorch 2.0å¼•å…¥çš„æ–°ç‰¹æ€§ï¼Œé€šè¿‡å³æ—¶ç¼–è¯‘æŠ€æœ¯æ˜¾è‘—æå‡æ¨¡å‹è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚ 

```python
config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]
exec(open('configurator.py').read()) # overrides from command line or config file
config = {k: globals()[k] for k in config_keys} # will be useful for logging
```
- `globals()` è¿”å›å½“å‰æ¨¡å—å…¨å±€å‘½åç©ºé—´çš„å­—å…¸ï¼Œ`python`ä¸­æ¯ä¸ªæ–‡ä»¶ä¼šæœ‰è‡ªå·±çš„å…¨å±€å‘½åç©ºé—´ï¼Œå­˜å‚¨äº†å®šä¹‰çš„æ‰€æœ‰å…¨å±€å˜é‡
- `config_keys`éå†æ‰€æœ‰å…¨å±€å˜é‡æ‰¾åˆ°æ‰€æœ‰config
- `exec(open('configurator.py').read())` è¯»å–`configurator.py`ä¸­çš„ä»£ç ç„¶åæ‰§è¡Œï¼Œä¿®æ”¹å…¨å±€å‘½åç©ºé—´ä¸­çš„å…¨å±€å˜é‡
- è¿™ä¸ªçš„å®ç°(åŸè¯)ä¸æ˜¯å¾ˆå¥½ï¼Œç›´æ¥importè™½ç„¶èƒ½æ‰§è¡Œä»£ç ä½†æ˜¯ä¸ä¼šä¿®æ”¹å½“å‰çš„globalsï¼Œå¦‚æœç”¨parserè§£æè¾“å…¥çš„å‚æ•°ï¼Œéœ€è¦å¯¹æ¯ä¸ªconfig variableéƒ½å†™ä¸€æ¬¡ï¼Œç¡®å®ä¹Ÿä¸æ˜¯å¾ˆå¥½


#### DDP 

```python
ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?
if ddp:
    init_process_group(backend=backend)
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    ddp_world_size = int(os.environ['WORLD_SIZE'])
    device = f'cuda:{ddp_local_rank}'
    torch.cuda.set_device(device)
    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.
    seed_offset = ddp_rank # each process gets a different seed
    # world_size number of processes will be training simultaneously, so we can scale
    # down the desired gradient accumulation iterations per process proportionally
    assert gradient_accumulation_steps % ddp_world_size == 0
    gradient_accumulation_steps //= ddp_world_size
else:
    # if not ddp, we are running on a single gpu, and one process
    master_process = True
    seed_offset = 0
    ddp_world_size = 1
    
tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size
print(f"tokens per iteration will be: {tokens_per_iter:,}")
```
- æ¯”è¾ƒéœ€è¦è¯´æ˜çš„åº”è¯¥æ˜¯
- ä¸ºæ¯ä¸ªprocessè®¾ç½®ä¸€ä¸ªseed
- `gradient_accumulation_steps` è¢«å„ä¸ªè¿›ç¨‹å¹³åˆ†
- `token_per_iter`è®¡ç®—äº†ä¸€æ¬¡è¿­ä»£è®­ç»ƒäº†å¤šå°‘token

```python
if master_process:
    os.makedirs(out_dir, exist_ok=True)
```
- master process ç”¨äºè®°å½•æ—¥å¿—ï¼Œä¿å­˜checkpoints   

```python
torch.manual_seed(1337 + seed_offset)
torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul
torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn
device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast
ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]
ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)
```
- `torch.autocast` æ˜¯pytorchçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºè‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ(Automatic Mixed Precision, AMP)ã€‚åœ¨æŸäº›æ“ä½œæ—¶ä½¿ç”¨ä½ç²¾åº¦(float16)åŠ é€Ÿè®¡ç®—ï¼Œå‡å°‘å†…å­˜å ç”¨ï¼Œåœ¨æŸäº›è®¡ç®—ä½¿ç”¨float32é¿å…æ•°å€¼ä¸ç¨³å®š
- `GradScaler` å½“ä½¿ç”¨float16æ—¶ï¼Œç”±äºç²¾åº¦è¾ƒä½ï¼Œå¯èƒ½å¯¼è‡´æ¢¯åº¦ä¸‹æº¢(å°æ•°å€¼è¢«æˆªæ–­æˆ–ä¸º0)ï¼Œéœ€è¦ä½¿ç”¨`GradScaler`ç¼©æ”¾æŸå¤±å€¼ã€‚æŒ‡å®š`autocast`çš„dtypeä¸ºfloat16æ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨
- `nullcontext()` è¿”å›ä¸€ä¸ªç©ºçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå®ƒæ˜¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä½†åˆä»€ä¹ˆéƒ½ä¸åšï¼Œç”¨äºå±•ä½æˆ–è€…ç®€åŒ–ä»£ç ï¼Œä¸`autocast`åº”æ˜¯æ­é…å‡ºç°ï¼ˆè¿™åº”è¯¥å¯ä»¥å½“æˆæ¿å­ï¼‰

#### Training

##### data loader

```python
def get_batch(split):
    # We recreate np.memmap every batch to avoid a memory leak, as per
    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122
    if split == 'train':
        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')
    else:
        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])
    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])
    if device_type == 'cuda':
        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)
        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)
    else:
        x, y = x.to(device), y.to(device)
    return x, y
```
- è¿™ä»ç„¶å’Œä¹‹å‰ä¸€æ ·æ˜¯ä¸€ä¸ªè‡ªå·±å®ç°çš„data loaderï¼Œéšæœºè·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
- ç”±äºæ•°æ®è¿‡å¤§ï¼Œä½¿ç”¨`memmap`å­˜å‚¨æ•°æ®
	- `memmap`ä¸å°†æ•°æ®ç›´æ¥å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œè€Œæ˜¯å­˜å‚¨åœ¨diskä¸­ï¼Œé€šè¿‡lazy loadingåŠ è½½åˆ°å†…å­˜ä¸­ã€‚æ­¤å¤„çš„å®ç°å‚è€ƒäº†[python - numpy memmap memory usage - want to iterate once - Stack Overflow](https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122)ï¼Œæ¯ä¸ªæ‰¹æ¬¡çš„sampleéƒ½åˆ›å»ºäº†ä¸€æ¬¡`memmap`ï¼Œé¿å…å†…å­˜æ³„æ¼
- å¦‚æœèƒ½å¤Ÿä½¿ç”¨`cuda`ï¼Œåˆ™å°†æ•°æ®æ”¾åˆ°GPUä¸­ã€‚`pin_memory`å°†æ•°æ®å›ºå®šåœ¨å†…å­˜ä¸­ä¸ä¼šè¢«æ“ä½œç³»ç»Ÿç½®æ¢å‡ºæ¥ï¼ŒåŠ ä¸Š`non_blocking=True`å®ç°äº†asynchronouslyè½¬ç§»åˆ°GPU

##### åˆå§‹åŒ–

```python
iter_num = 0
best_val_loss = 1e9
```
- åˆå§‹åŒ–è¿­ä»£æ¬¡æ•°å’Œæœ€ä½³çš„loss

```python
meta_path = os.path.join(data_dir, 'meta.pkl')
meta_vocab_size = None
if os.path.exists(meta_path):
    with open(meta_path, 'rb') as f:
        meta = pickle.load(f)
    meta_vocab_size = meta['vocab_size']
    print(f"found vocab_size = {meta_vocab_size} (inside {meta_path})")
```
- ä»æ•°æ®é›†çš„`meta.pkl`å¦‚æœæœ‰çš„è¯å¯»æ‰¾`vocab_size`

**åˆå§‹åŒ–æ¨¡å‹**   
```python
# model init
model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,
                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line
if init_from == 'scratch':
    # init a new model from scratch
    print("Initializing a new model from scratch")
    # determine the vocab size we'll use for from-scratch training
    if meta_vocab_size is None:
        print("defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)")
    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304
    gptconf = GPTConfig(**model_args)
    model = GPT(gptconf)
elif init_from == 'resume':
    print(f"Resuming training from {out_dir}")
    # resume training from a checkpoint.
    ckpt_path = os.path.join(out_dir, 'ckpt.pt')
    checkpoint = torch.load(ckpt_path, map_location=device)
    checkpoint_model_args = checkpoint['model_args']
    # force these config attributes to be equal otherwise we can't even resume training
    # the rest of the attributes (e.g. dropout) can stay as desired from command line
    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:
        model_args[k] = checkpoint_model_args[k]
    # create the model
    gptconf = GPTConfig(**model_args)
    model = GPT(gptconf)
    state_dict = checkpoint['model']
    # fix the keys of the state dictionary :(
    # honestly no idea how checkpoints sometimes get this prefix, have to debug more
    unwanted_prefix = '_orig_mod.'
    for k,v in list(state_dict.items()):
        if k.startswith(unwanted_prefix):
            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)
    model.load_state_dict(state_dict)
    iter_num = checkpoint['iter_num']
    best_val_loss = checkpoint['best_val_loss']
elif init_from.startswith('gpt2'):
    print(f"Initializing from OpenAI GPT-2 weights: {init_from}")
    # initialize from OpenAI GPT-2 weights
    override_args = dict(dropout=dropout)
    model = GPT.from_pretrained(init_from, override_args)
    # read off the created config params, so we can store them into checkpoint correctly
    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:
        model_args[k] = getattr(model.config, k)
# crop down the model block size if desired, using model surgery
if block_size < model.config.block_size:
    model.crop_block_size(block_size)
    model_args['block_size'] = block_size # so that the checkpoint will have the right value
model.to(device)
```
- `model_args` ä½œä¸º`GPTconfig`çš„å‚æ•°
- `scratch`ä¸ºä»å¤´åˆå§‹åŒ–æ¨¡å‹ï¼Œ`resume`ä¸ºä»checkpointæ¢å¤
- `checkpoint = torch.load(ckpt_path, map_location=device)` ä¸­`map_location`å°†å¼ é‡åŠ è½½åˆ°`device`ä¸­
- ä¸­é—´å»æ‰ä¸éœ€è¦çš„å‰ç¼€åº”è¯¥æ˜¯è¯•é”™å¾—åˆ°çš„ï¼Œéœ€è¦ç•™æ„ä¸€ä¸‹(æŒ‡æŠ„ä¸€ä¸‹)ï¼Œè¿™é‡Œä¸ºä»€ä¹ˆä¸ç›´æ¥ä»checkpointåŠ è½½å‘¢ï¼Ÿ
- æœ€åå¦‚æœç”¨GPT2çš„æƒé‡è¿›è¡Œåˆå§‹åŒ–ï¼Œåªéœ€è¦`model = GPT.from_pretrained(init_from, override_args)`ï¼Œä¸ºäº†æ­£ç¡®checkpointï¼Œéœ€è¦å°†æ¨¡å‹configè½¬ç§»åˆ°å­—å…¸`model_args`
- æŒ‰éœ€ä¿®æ”¹ä¸€ä¸‹`block_size`
- å°†æ¨¡å‹è½¬ç§»åˆ°deviceä¸­

```python
# initialize a GradScaler. If enabled=False scaler is a no-op
scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
```
- åˆå§‹åŒ–`scaler`ï¼Œ`scaler`ä½œç”¨è§ä¸Šæ–‡

**ä¼˜åŒ–å™¨**   
```python
# optimizer
optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)
if init_from == 'resume':
    optimizer.load_state_dict(checkpoint['optimizer'])
checkpoint = None # free up memory
```
- è°ƒç”¨`model`ä¸­å®ç°çš„`configure_optimizers`å¾—åˆ°optimizer
- å¦‚æœæ˜¯æ¢å¤çš„éœ€è¦æ¢å¤optimizerçŠ¶æ€
- é‡Šæ”¾checkpointå˜é‡

**Compile**  
```python
# compile the model
if compile:
    print("compiling the model... (takes a ~minute)")
    unoptimized_model = model
    model = torch.compile(model) # requires PyTorch 2.0
```
- `torch.compile` ç”Ÿæˆä¼˜åŒ–åçš„è®¡ç®—å›¾ï¼Œä¸åŸåœ°ä¿®æ”¹æ¨¡å‹ï¼Œ`unoptimized_model`è¿™é‡Œåº”è¯¥æ˜¯ç»´æŠ¤ä¸€ä¸ªæœªä¼˜åŒ–çš„æ¨¡å‹å‰¯æœ¬ï¼Œè™½ç„¶åé¢çš„ä»£ç æ²¡ç”¨åˆ°è¿™ä¸ª
- åº”è¯¥æ˜¯ä¸€ç§æ¿å­

```python
# wrap model into DDP container
if ddp:
    model = DDP(model, device_ids=[ddp_local_rank])
```
- åŠ ä¸ŠDDPå®¹å™¨

##### åŠŸèƒ½å‡½æ•°

```python
# helps estimate an arbitrarily accurate loss over either split using many batches
@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            with ctx:
                logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out
```
- è¿­ä»£`eval_iters`æ¬¡ï¼Œå¾—åˆ°ç›¸åº”æ¬¡æ•°çš„batchæ¥è·å¾—æ›´ç²¾ç¡®çš„å¹³å‡lossï¼Œè¿™ä¸ªå¯èƒ½ä¼šæ¶‰åŠåˆ°è®­ç»ƒæ•°æ®çš„å¤§å°è®¡ç®—ï¼ŒæŒ‰ç†è¯´è¦`eval_iters`ä¹˜ä»¥`batch`è¦å°äºæ€»æ•°æ®é‡æ¯”è¾ƒå¥½(æ¢æ•°æ®é›†éœ€è¦æ³¨æ„)
- `ctx` æ˜¯ä¸Šæ–‡å®šä¹‰çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå¦‚æœæœ‰GPUçš„è¯å°±æ˜¯`torch.cuda.amp.autocast`ï¼Œç”¨FP16åŠ é€Ÿè®¡ç®—

```python
# learning rate decay scheduler (cosine with warmup)
def get_lr(it):
    # 1) linear warmup for warmup_iters steps
    if it < warmup_iters:
        return learning_rate * (it + 1) / (warmup_iters + 1)
    # 2) if it > lr_decay_iters, return min learning rate
    if it > lr_decay_iters:
        return min_lr
    # 3) in between, use cosine decay down to min learning rate
    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)
    assert 0 <= decay_ratio <= 1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1
    return min_lr + coeff * (learning_rate - min_lr)
```
- å½“å°äº`warmup`æ­¥æ•°æ—¶linear warmupï¼Œå½“å¤§äºlearning rate decayæ­¥æ•°æ—¶ç›´æ¥è¿”å›æœ€å°lrï¼Œä¸­é—´ä½¿ç”¨cosine decay
- `+1` é¿å…åˆå§‹å­¦ä¹ ç‡ä¸º0
- `dacay_ratio` è®¡ç®—å½“å‰è¿­ä»£æ¬¡æ•°å æ¯”ï¼Œç”¨äºè°ƒæ•´cosineçš„å˜åŒ–ï¼Œ`0.5`å³1/2ï¼Œé™åˆ¶å­¦ä¹ ç‡å¤§å°åœ¨0,1ä¹‹é—´ï¼Œä»0åˆ°$\pi$ï¼Œä»1åˆ°-1

```python
# logging
if wandb_log and master_process:
    import wandb
    wandb.init(project=wandb_project, name=wandb_run_name, config=config)
```
- ç”¨`wandb`è®°å½•æ—¥å¿—(å¾—æŸ¥ä¸€ä¸‹autodlæ€ä¹ˆç”¨wandbå¯èƒ½)

##### Trainning loop

```python
X, Y = get_batch('train') # fetch the very first batch
t0 = time.time()
local_iter_num = 0 # number of iterations in the lifetime of this process
raw_model = model.module if ddp else model # unwrap DDP container if needed
running_mfu = -1.0
```
- è¿›è¡Œç®€å•çš„åˆå§‹åŒ–

```python
while True:

    # determine and set the learning rate for this iteration
    lr = get_lr(iter_num) if decay_lr else learning_rate
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

    # evaluate the loss on train/val sets and write checkpoints
    if iter_num % eval_interval == 0 and master_process:
        losses = estimate_loss()
        print(f"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")
        if wandb_log:
            wandb.log({
                "iter": iter_num,
                "train/loss": losses['train'],
                "val/loss": losses['val'],
                "lr": lr,
                "mfu": running_mfu*100, # convert to percentage
            })
        if losses['val'] < best_val_loss or always_save_checkpoint:
            best_val_loss = losses['val']
            if iter_num > 0:
                checkpoint = {
                    'model': raw_model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'model_args': model_args,
                    'iter_num': iter_num,
                    'best_val_loss': best_val_loss,
                    'config': config,
                }
                print(f"saving checkpoint to {out_dir}")
                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))
    if iter_num == 0 and eval_only:
        break
```
- å…ˆå¾—åˆ°lrå¹¶è®°å½•
	- æ ¹æ®æ€»å…±çš„`iter_num`è·å¾—å­¦ä¹ ç‡
	- `optimizer.param_groups`   å¯¹optimizerçš„å„ä¸ªå‚æ•°è¿›è¡Œåˆ†ç»„ç®¡ç†ï¼Œä¾‹å¦‚ï¼šåˆ†å±‚ï¼Œå†»ç»“å‚æ•°ï¼Œä¸åŒåˆ†å±‚ä¸åŒå­¦ä¹ ç‡ï¼Œä¸åŒåˆ†å±‚ä¸åŒè¡°å‡
	- è¿™é‡ŒæŠŠæ‰€æœ‰ç»„çš„å­¦ä¹ ç‡éƒ½è®¾ä¸ºè·å¾—çš„`lr`
- æ¯é—´éš”`eval_interval`æ¬¡è¿­ä»£å’Œä¸ºä¸»è¿›ç¨‹æ—¶ï¼Œè®¡ç®—lossï¼Œè®°å½•æ—¥å¿—å’Œcheckpoint
- å¦‚æœåªæ˜¯evalï¼Œè®¡ç®—å®Œlossï¼Œä¿å­˜åç›´æ¥é€€å‡º

**forward and backward**   
```python
for micro_step in range(gradient_accumulation_steps):
	if ddp:
		# in DDP training we only need to sync gradients at the last micro step.
		# the official way to do this is with model.no_sync() context manager, but
		# I really dislike that this bloats the code and forces us to repeat code
		# looking at the source of that context manager, it just toggles this variable
		model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)
	with ctx:
		logits, loss = model(X, Y)
		loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation
	# immediately async prefetch next batch while model is doing the forward pass on the GPU
	X, Y = get_batch('train')
	# backward pass, with gradient scaling if training in fp16
	scaler.scale(loss).backward()
```
- å¦‚æœæ˜¯ddpï¼Œéœ€è¦åœ¨ä¸€æ¬¡ç´¯ç§¯æ¢¯åº¦çš„æœ€åä¸€ä¸ªstepåŒæ­¥
	- ä¸ºä»€ä¹ˆæ˜¯æœ€åä¸€ä¸ªstep?
	- æˆ‘ä»¬éœ€è¦çš„æ˜¯æ‰€æœ‰processä¸Šæ‰€æœ‰gradient_accumulation_stepsä¸ªbatchåç´¯åŠ çš„å¹³å‡ï¼Œæ€»å…±åº”æ˜¯ gradient_accumulation_steps * number_of_process(world_size) * batch_size ä¸ºä¸€ä¸ªå¤§batch
	- åŒæ­¥æ˜¯åŠ å’Œå¹³å‡ï¼Œå¦‚æœæ¯æ¬¡éƒ½åŒæ­¥ï¼Œä¼šå¤šæ¬¡å¹³å‡å¾—åˆ°é”™è¯¯ç»“æœ
- æ¯ä¸ªprocessæ¯æ¬¡å¾ªç¯è®¡ç®—ä¸€æ¬¡losså’Œè‡ªå·±çš„æ¢¯åº¦
	- ä¸ºä»€ä¹ˆlossè¦é™¤ä»¥gradient_accumulation_steps
	- ä¿è¯æ¢¯åº¦ä¸ä¼šå¤ªå¤§(æ¢¯åº¦çˆ†ç‚¸)
- æ•°æ®çš„è·å–æ˜¯å¼‚æ­¥çš„ï¼Œæ‰€ä»¥åœ¨å‰ä¼ çš„æ—¶å€™å¯ä»¥æ‰§è¡Œä¸‹ä¸€ä¸ªbatchæ•°æ®çš„è·å–

```python
# clip the gradient
if grad_clip != 0.0:
	scaler.unscale_(optimizer)
	torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
```
- `unscale_(optimizer)` å°†scaleræ”¾å¤§losså¯¼è‡´æ”¾å¤§çš„æ¢¯åº¦è¿˜åŸ
- `torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)` 
	- è£å‰ªæ‰€æœ‰å‚æ•°çš„æ¢¯åº¦èŒƒæ•°(L2èŒƒæ•°)ï¼Œå¦‚æœè¶…è¿‡è®¾å®šçš„é˜ˆå€¼ï¼Œæ¢¯åº¦ä¼šæŒ‰ç…§æ¯”ä¾‹ `é˜ˆå€¼/å®é™…å€¼`æ•´ä½“ç¼©æ”¾
	- ä¸ºä»€ä¹ˆè¦å…ˆè¿˜åŸå†è£å‰ªï¼Ÿåç»­ä½¿ç”¨`scaler`è¿›è¡Œå‚æ•°æ›´æ–°çš„æ—¶å€™ä¼šè¿˜åŸä¸€æ¬¡æ¢¯åº¦ï¼Œå¦‚æœè¿™é‡Œä¸å…ˆè¿˜åŸå†è£å‰ªï¼Œä¼šå¯¼è‡´`scaler`æ”¾å¤§çš„å€æ•°åœ¨è¿™é‡Œå…ˆè¢«é™¤ä¸€æ¬¡ï¼Œå¯¼è‡´ä¸¤æ¬¡ç¼©å°
	- å¦‚æœå…ˆç¼©æ”¾å†æ›´æ–°`scaler.step(optimizer)`ï¼Œå› ä¸ºå·²ç»ä½¿ç”¨è¿‡`unscale_`ï¼Œä¼šç›´æ¥è°ƒç”¨`optimizer`ï¼Œä¸ä¼šå†ç¼©æ”¾ä¸€æ¬¡

>å…³äºæ··åˆç²¾åº¦è®¡ç®—çš„æ•°æ®ç±»å‹é—®é¢˜ï¼š
>å¦‚æœ`autocast`é€‰æ‹©`float16`ï¼Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„çŸ©é˜µè®¡ç®—ä¼šç”¨`float16`è®¡ç®—ï¼Œç”¨äºåŠ é€Ÿã€‚æ­¤æ—¶åå‘è®¡ç®—æ¢¯åº¦éœ€è¦ä½¿ç”¨`scaler.scale(loss).backward()`æ¥æ”¾å¤§lossï¼Œä¿è¯ä¸ä¼šå‡ºç°æ¢¯åº¦ä¸‹æº¢
>ä½†æ˜¯æ”¾å¤§åçš„æ¢¯åº¦å­˜åœ¨`float32`ä¸­ï¼Œä¸€ç›´åˆ°`scaler.step(optimizer)`ï¼Œä¼šå°†æ”¾å¤§çš„æ¢¯åº¦ç¼©æ”¾ä¸ºåŸæ¥å¤§å°

```python
# step the optimizer and scaler if training in fp16
scaler.step(optimizer)
scaler.update()
# flush the gradients as soon as we can, no need for this memory anymore
optimizer.zero_grad(set_to_none=True)
```
- æ›´æ–°å‚æ•°
- é‡Šæ”¾æ¢¯åº¦å ç”¨çš„å†…å­˜


```python
# timing and logging
t1 = time.time()
dt = t1 - t0
t0 = t1
if iter_num % log_interval == 0 and master_process:
	# get loss as float. note: this is a CPU-GPU sync point
	# scale up to undo the division above, approximating the true total loss (exact would have been a sum)
	lossf = loss.item() * gradient_accumulation_steps
	if local_iter_num >= 5: # let the training loop settle a bit
		mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)
		running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu
	print(f"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%")
iter_num += 1
local_iter_num += 1

# termination conditions
if iter_num > max_iters:
	break
```
- è®°å½•æ—¶é—´å’Œmfu
- ä¸ºä»€ä¹ˆ`running_mfu`è¿™ä¹ˆç®—ï¼Ÿ è¿™æ˜¯ä¸€ä¸ª**æŒ‡æ•°ç§»åŠ¨å¹³å‡(Exponential Moving Average, EMA)**ï¼ŒEMAå¯¹è¿‘æœŸæ•°æ®æ›´æ•æ„Ÿï¼Œæ›´èŠ‚çœè®¡ç®—èµ„æº


```python
if ddp:
    destroy_process_group()
```
- é”€æ¯è¿›ç¨‹ç»„ï¼Œé‡Šæ”¾èµ„æº

## `sample.py`

å¯¼å…¥å¿…è¦çš„åº“   
```python
import os
import pickle
from contextlib import nullcontext
import torch
import tiktoken
from model import GPTConfig, GPT
```
- `pickle` æ˜¯pythonæ ‡å‡†åº“ä¸­ç”¨äºå¯¹è±¡åºåˆ—åŒ–ä¸ååºåˆ—åŒ–çš„æ¨¡å—ï¼Œåºåˆ—åŒ–æŒ‡å°†å¯¹è±¡è½¬æ¢ä¸ºå­—èŠ‚æµï¼Œååºåˆ—åŒ–æŒ‡ä»å­—èŠ‚æµä¸­æ¢å¤å¯¹è±¡ã€‚æ–‡ä»¶åç¼€åä¸º`.pkl`  
- `tiktoken` æ˜¯OpenAIçš„tokenizer

è¶…å‚æ•°å’Œå…¨å±€å˜é‡   
```python
init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')
out_dir = 'out' # ignored if init_from is not 'resume'
start = "\n" # or "<|endoftext|>" or etc. Can also specify a file, use as: "FILE:prompt.txt"
num_samples = 10 # number of samples to draw
max_new_tokens = 500 # number of tokens generated in each sample
temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions
top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability
seed = 1337
device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.
dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'
compile = False # use PyTorch 2.0 to compile the model to be faster
exec(open('configurator.py').read()) # overrides from command line or config file
```
- è¯¥æ–‡ä»¶ç”¨äºç”Ÿæˆï¼Œæ‰€ä»¥åªèƒ½æ˜¯resumeæˆ–gpt2
- `start` æ˜¯èµ·å§‹ç¬¦å·
- `top_k` ä½¿ç”¨top-k sample
- `bfloat16` BF16ï¼Œæ˜¯è°·æ­Œæå‡ºçš„ç”¨äºæ·±åº¦å­¦ä¹ çš„ä¼˜åŒ–ç‰ˆFP16ï¼ŒFP16æœ‰1ä½ç¬¦å·ä½ï¼Œ5ä½æŒ‡æ•°ä½ï¼Œ10ä½å°¾æ•°ä½ï¼Œè€ŒBF16æœ‰8ä½æŒ‡æ•°ä½(ä¸FP32ç›¸åŒ)ï¼Œ7ä½å°¾æ•°ä½(ç²¾åº¦è¾ƒä½)ã€‚BF16èŒƒå›´æ›´å¤§ï¼Œç²¾åº¦è¾ƒä½ï¼Œéœ€ç‰¹å®šç¡¬ä»¶æ”¯æŒ
- `exec(open('configurator.py').read())` ä¿®æ”¹å‚æ•°ï¼Œè§`train.py`çš„è§£é‡Š

>##### å…³äºæµ®ç‚¹æ•°ç²¾åº¦
>
>- åŠç²¾åº¦æ˜¯æŒ‡float16(FP16)ï¼Œ1+5+10ï¼Œå†…å­˜å ç”¨å°ï¼Œå¯èƒ½æœ‰ç²¾åº¦é—®é¢˜ï¼ŒGPUå¤„ç†é€Ÿåº¦å¿«
>- å•ç²¾åº¦æ˜¯æŒ‡float32(FP32)ï¼Œ1+8+23ï¼Œå¤§å¤šæ•°æ¡†æ¶çš„é»˜è®¤æµ®ç‚¹æ•°
>- åŒç²¾åº¦æ˜¯æŒ‡float64(FP64)ï¼Œ1+11+52ï¼Œç”¨äºé«˜ç²¾åº¦ä»»åŠ¡
>- bfloat16(BF16)ï¼Œ1+8+7ï¼ŒæŒ‡æ•°ä½(èŒƒå›´)ä¸FP32ç›¸åŒï¼Œç²¾åº¦ä½äºFP16ï¼Œé€‚ç”¨äºä¸FP32æ— ç¼åˆ‡æ¢ï¼Œéœ€è¦ç¡¬ä»¶æ”¯æŒ
>- TensorFloat32(TF32)ï¼Œ1+8+10ï¼ŒTF32æ˜¯NVIDIAåœ¨Ampereæ¶æ„GPU(A100ï¼ŒRTX30ç³»åˆ—)å¼•å…¥çš„æ•°æ®ç±»å‹ï¼ŒæŒ‡æ•°ä½ä¸FP32ä¸€è‡´ï¼Œç²¾åº¦ä¸åŠç²¾åº¦ä¸€è‡´ï¼Œé¿å…FP16çš„æº¢å‡ºé—®é¢˜ï¼Œä½¿ç”¨tensor coreåŠ é€Ÿï¼Œä¸FP16é€Ÿåº¦æ¥è¿‘ã€‚TF32è™½ç„¶åªæœ‰19ä½ï¼Œä½†æ˜¯å ç”¨32ä½å†…å­˜ç©ºé—´

```python
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul
torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn
device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast
ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]
ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)
```
- `torch.manual_seed()` 
- `torch.manual_seed()` è®¾ç½®äº†CPUçš„éšæœºç§å­ï¼Œ`torch.cuda.manual_seed()` è®¾ç½®äº†GPUçš„éšæœºç§å­
- `torch.backends.cuda.matmul.allow_tf32 = True`å’Œ`torch.backends.cudnn.allow_tf32 = True` æ˜¾å¼å¯ç”¨TF32åŠ é€Ÿï¼Œä¿è¯ä¸åŒç¯å¢ƒçš„ä¸€è‡´æ€§(Ampereæ¶æ„ä¸­é»˜è®¤å¯ç”¨)


>##### å…³äºéšæœºç§å­
>ä¸€ä¸ªæ·±åº¦å­¦ä¹ ç¨‹åºä¸­å¯èƒ½éœ€è¦è®¾ç½®å¤šç§éšæœºç§å­
>- CPU ä½¿ç”¨ `torch.manual_seed(seed)`
>- GPUä½¿ç”¨ `torch.cuda.manual_seed(seed)`ï¼ŒåŒæ—¶æ¯ä¸ªGPUæœ‰è‡ªå·±çš„ç§å­ç”Ÿæˆå™¨ï¼Œå¦‚æœè¦è®¾ç½®æ‰€æœ‰GPUåˆ™ä½¿ç”¨ `torch.cuda.manual_seed_all(seed)`
>- å•çº¯è®¾ç½®ä¸Šè¿°ä¸¤ç§ç§å­ï¼Œæœ‰äº›GPUæ“ä½œä»å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œå¦‚cuDNNçš„å·ç§¯ï¼Œéœ€é¢å¤–è®¾ç½®`torch.backends.cudnn.deterministic = True` å¼ºåˆ¶ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•ï¼Œ`torch.backends.cudnn.benchmark = False` å…³é—­è‡ªåŠ¨ä¼˜åŒ–
>- å¦‚æœä½¿ç”¨äº†`numpy`å’Œ`random`ï¼Œéœ€è¦é¢å¤–è®¾ç½®`random.seed(seed)`ï¼Œ`np.random.seed(seed)`

åŠ è½½æ¨¡å‹  
```python
# model
if init_from == 'resume':
    # init from a model saved in a specific directory
    ckpt_path = os.path.join(out_dir, 'ckpt.pt')
    checkpoint = torch.load(ckpt_path, map_location=device)
    gptconf = GPTConfig(**checkpoint['model_args'])
    model = GPT(gptconf)
    state_dict = checkpoint['model']
    unwanted_prefix = '_orig_mod.'
    for k,v in list(state_dict.items()):
        if k.startswith(unwanted_prefix):
            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)
    model.load_state_dict(state_dict)
elif init_from.startswith('gpt2'):
    # init from a given GPT-2 model
    model = GPT.from_pretrained(init_from, dict(dropout=0.0))

model.eval()
model.to(device)
if compile:
    model = torch.compile(model) # requires PyTorch 2.0 (optional)
```
- è§£é‡Šè§`train.py`

```python
# look for the meta pickle in case it is available in the dataset folder
load_meta = False
if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...
    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')
    load_meta = os.path.exists(meta_path)
if load_meta:
    print(f"Loading meta from {meta_path}...")
    with open(meta_path, 'rb') as f:
        meta = pickle.load(f)
    # TODO want to make this more general to arbitrary encoder/decoder schemes
    stoi, itos = meta['stoi'], meta['itos']
    encode = lambda s: [stoi[c] for c in s]
    decode = lambda l: ''.join([itos[i] for i in l])
else:
    # ok let's assume gpt-2 encodings by default
    print("No meta.pkl found, assuming GPT-2 encodings...")
    enc = tiktoken.get_encoding("gpt2")
    encode = lambda s: enc.encode(s, allowed_special={"<|endoftext|>"})
    decode = lambda l: enc.decode(l)
```
- `meta.pkl` å­˜å‚¨ç€å¤„ç†æ•°æ®æ—¶ä¿å­˜çš„encoderï¼Œdecoderï¼Œåœ¨è¿™é‡Œå¯èƒ½æ˜¯æ•™ç¨‹ä¸­çš„character tokenizer
- å¦‚æœæ²¡æœ‰çš„è¯å°±é»˜è®¤tiktoken

```python
# encode the beginning of the prompt
if start.startswith('FILE:'):
    with open(start[5:], 'r', encoding='utf-8') as f:
        start = f.read()
start_ids = encode(start)
x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])
```
- å¦‚æœèµ·å§‹çš„promptæ¥è‡ªæ–‡ä»¶ï¼Œåˆ™å…ˆè¯»å–prompt
- encode promptä½œä¸ºè¾“å…¥x

sample   
```python
# run generation
with torch.no_grad():
    with ctx:
        for k in range(num_samples):
            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)
            print(decode(y[0].tolist()))
            print('---------------')
```
- æ³¨æ„decodeçš„æ—¶å€™éœ€è¦å°†è¾“å‡ºè½¬æ¢ä¸º æ•´æ•°åˆ—è¡¨
- tiktokenå·¥ä½œåœ¨CPUä¸Š














































































