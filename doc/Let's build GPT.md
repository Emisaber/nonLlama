## References

[Let's build GPT: from scratch, in code, spelled out. - YouTube](https://www.youtube.com/watch?v=kCc8FmEb1nY)   

## Reading and exploring the data

tokenizer map the characters/words to integer/code  
In character size tokenizer the len of chars in dataset is the vocabulary size of model   
so with character size tokenizer, we have small size of code book but long sequence of encode   
![[Pasted image 20250110160507.png]]  
#### tokenizer
nanoGPT use the very simple tokenizer   
```python
stoi = {ch:i for ch, i in enumerate(chars)}
itos = {i:ch for i, ch in enumerate(chars)}

encode = lambda s: [stoi[i] for i in s]
decode = lambda l: "".join([itos[i] for i in l])
``` 

> ÊÑüËßâÔºåËøôÈáåÁî®lambda ‰∏ªË¶ÅÊòØ‰∏∫‰∫ÜÁúÅË°åÊï∞ÔºåÂÆûÈôÖÂÜôÁöÑÊó∂ÂÄôÂèØ‰ª•Êîπ‰∏Ä‰∏ã

#### data split

Encode all the data(shakespere works here)   
split training dataset and validation dataset.   
0.9 as training data, and the rest as validation data   

> Â∞ΩÁÆ°ËøôÂè™ÊòØ‰∏Ä‰∏™Ê†∑‰æãÔºå‰ΩÜÊòØËééÂ£´ÊØî‰∫öÁöÑ‰ΩúÂìÅÂ¶ÇÊûúÊòØÊúâÂ∫èÁöÑÔºåËøô‰πàÂàíÂàÜÊúâÁÇπÂ•áÊÄ™  

training on chunks of text    
the chunks of text size `block_size` is also nemed context size   
```python
x = train_data[:block_size]
y = train_data[1:block_size+1]  # one offset on target
```

**Ê≥®ÊÑèyÊØîxÂêëÂè≥ÂÅèÁßª‰∫Ü1‰ΩçÔºå‰∏ã‰∏Ä‰ΩçÊâçÊòØtarget**   

The input shoulbe be a sequence of context, and the output is the next token    
```python
for t in range(block_size):
	context = x[:t+1]
	target = y[t]
	print(f"when the input is {context}, the output should be {target}")
```

> ÂÜçÊ¨°Ê≥®ÊÑèËæìÂÖ•**‰ªé1‰ΩçÂà∞Êï¥‰∏™‰∏ä‰∏ãÊñáÈïøÂ∫¶**(`block_size`)    
> ËøôÊ†∑transformerËÉΩÂ§ü‰ªé‰∏Ä‰ΩçÂºÄÂßãpredictÁõ¥Âà∞block_size  
> Êú¨Ë∫´ÁöÑËæìÂÖ•ÊúÄÈïøÂ∞±ÊòØblock_sizeÔºåÊâÄ‰ª•‰∏ÄÁõ¥Âà∞Êï¥‰∏™ÈïøÂ∫¶‰Ωú‰∏∫ËæìÂÖ•ËøõË°åËÆ≠ÁªÉ‰πüÂæàÂêàÁêÜ

Consider the batch dimension    
```python
def get_batch(split):
	data = train_data if split == "train" else val_data
	idx = torch.randint(len(data) - block_size, (batch_size))
	x = torch.stack([data[i:block_size] for i in idx])
	y = torch.stack([data[i+1:block_size+1] for i in idx])
	return x, y
```

> ‰∏™‰∫∫ÊÑüËßâÊòØÂæàÂ∑ßÂ¶ôÁöÑÂÆûÁé∞ÔºåÈöèÊú∫ÈÄâÂèñidxËé∑ÂæóblockÔºåËøô‰ΩïÂ∞ù‰∏çÊòØ‰∏ÄÁßçËíôÁâπÂç°Ê¥õ  
> ‰ΩÜÊòØËøôÊ†∑Â∫îËØ•Ê≤°ÂäûÊ≥ïÁ°Æ‰øùÊâÄÊúâÊï∞ÊçÆÈÉΩË¢´‰ΩøÁî®ÔºåÁúüÂÆûÁöÑbatchÊòØÂ¶Ç‰ΩïÂÆûÁé∞ÁöÑ üëàÁúãÁúãd2l

ÂæóÂà∞ÁöÑxÔºåyÊòØÁõ∏ÂØπÂ∫îÁöÑ(4, 8)ÁöÑ‰∫åÁª¥Êï∞ÁªÑ   
Ê≥®ÊÑèËøôÂØπÂ∫î‰∫Ü32ÂØπËÆ≠ÁªÉÊï∞ÊçÆ(every position, independent as far as the transformer concerned)       
32ÂØπÊï∞ÊçÆÊòØ‰∏Ä‰∏™batch size ‰∏∫4ÁöÑ‰∏Ä‰∏™batchÁöÑËæìÂÖ•ÔºåËÆ≠ÁªÉÊó∂ÂØπtransformerÊù•ËØ¥Áõ∏ÂΩì‰∫é‰∏Ä‰∏™batch‰∏∫32(32ÂØπÊï∞ÊçÆËÆ°ÁÆóloss)ÔºåËøôÊòØÈÄöËøáÊé©Á†ÅÂÆûÁé∞ÁöÑ   

## Model

### Build BigramLanguageModel
using pytorch for efficiency  

```python
class BigramLanguageModel(nn.Module):

	def __init__(self, vocab_size):
		super().__init__()
		self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

	def forward(self, idx, targets):
		logits = self.token_embedding_table(idx) # B, T, C
		B, T, C = logits.shape
		logits = logits.view(B*T, C)
		targets = targets.view(B*T)
		loss = F.cross_entropy(logits, targets)  # use nn.functional 

		return logits, loss
```

#### Some Supplement

##### What is BigramLanguageModel?  

Generally speaking, bigram language model is the model that predict next token only depend on the last token (or words pair)   
`BigramLanguageModel` here performs as a baseline   
more details see [[makemore]] üëà **ÂæÖË°•**    

##### Why reshape?

ËæìÂÖ•ÊòØ‰∏Ä‰∏™ `B, T` ÁöÑÂêëÈáèÔºåBÊòØ`batch_size`, TÊòØtime step      
ÁªèËøáembeddingÂêéÔºålogitsÊòØ‰∏Ä‰∏™`B, T, C` ÁöÑÂêëÈáèÔºåÂÖ∂‰∏≠CÊòØchannelÔºåÂç≥`vocab_size`   
`torch.nn.Cross_Entropy` (a class by the way) Êé•Âèó`B, C` ÁöÑËæìÂÖ•ÔºåË¶ÅÊ±ÇchannelÂú®Á¨¨‰∫å‰Ωç   
> `torch.nn.cross_entropy`  is the functional version

So reshape is needed  
What we care about is the loss of the whole input(batch), so we directly change the size into `B*T`, make the batch size for cross-entropy become all input(32 rows here)   
##### What does the `cross-entropy` do

~~Âà∞ËøôÈáåÂ∞±ÂæàÈöæÂÅáË£Ö‰ºöËã±ËØ≠‰∫Ü~~    
**Êï∞Â≠¶‰∏ä**ÔºåCross-entropy ÂÜô‰Ωú   

$$
\mathrm{C r o s sE n t r o p y}=-\sum_{i=1}^{C} y_{i} \operatorname{l o g} ( p_{i} ) 
$$
ÂÖ∂‰∏≠Ôºå$y_i$ÊòØÁõÆÊ†áÂàÜÂ∏ÉÁöÑÊ¶ÇÁéáÔºå$p_i$ÊòØÈ¢ÑÊµãÂàÜÂ∏ÉÁöÑÊ¶ÇÁéá   
ÂΩìÁî®‰∫éÁ±ªÂà´ËÆ°ÁÆóÊó∂ÔºåÁõÆÊ†áÂàÜÁ±ªË°®Á§∫‰∏∫`[0, 1, 2]`ÔºåÂç≥Á¨¨‰∏Ä‰∏™Ê†∑Êú¨‰∏∫0Á±ªÔºåÁ¨¨‰∫å‰∏™‰∏∫1Á±ªÔºå‰ª•Ê≠§Á±ªÊé®   
ËæìÂÖ•ÊòØ‰∏Ä‰∏™‰∫åÁª¥Êï∞ÁªÑÔºåÊØè‰∏Ä‰∏™Ê†∑Êú¨ÊòØÂú®ÂêÑ‰∏™Á±ªÂà´ÁöÑÊú™ÂΩí‰∏ÄÂåñÁöÑ**logits**    
```
[[0.2, 2.0, 1,2],
[0.3, 3.0, 0.4],
[0.6, 1.0, 1.5]]
```

ÁõÆÊ†áÁöÑÂàÜÂ∏ÉÔºåÂÆûÈôÖ‰∏äÂè™Âú®Ê≠£Á°ÆÊ†∑Êú¨‰∏äÊ¶ÇÁéá‰∏∫1ÔºåÂÖ∂‰ΩôÈÉΩ‰∏∫0   
```
[[0, 1, 0],
[0, 1, 0],
[0, 0, 1]]
```

ÊâÄ‰ª•Âú®ÂàÜÁ±ª‰ªªÂä°ËÆ°ÁÆócross-entropyÔºåÂè™ÈúÄË¶ÅËÆ°ÁÆóÊ≠£Á°ÆÁ±ª‰∏äÁöÑ **negative log likelihood(NLL)**   

$$
\mathrm{N L L}=-\operatorname{l o g} ( p_{\mathrm{c o r r e c t}} ) 
$$

Â¶ÇÊûúÂ§ö‰∏™Ê†∑Êú¨/(batch size here)   
ÈÇ£Â∞±ÊòØlog likelihood Á¥ØÂä†ÂÜçÂπ≥Âùá      

$$
\mathrm{B a t c h} \; \mathrm{L o s s}=\frac{1} {N} \sum_{n=1}^{N} \mathrm{L o s s}_{n} 
$$

**‰ª£Á†Å‰∏ä**    
`cross_entropy`ÂÜÖÁΩÆsoftmaxÔºåËæìÂÖ•Êú™ÂΩí‰∏ÄÂåñÁöÑ`logits`Âíå‰∏Ä‰∏ÄÂØπÂ∫îÁöÑintegerÂΩ¢ÂºèÁöÑ`labels`  
- ËøõË°åsoftmax‰πãÂêé
- Êü•ÊâæÂØπÂ∫îÁöÑÊ≠£Á°ÆÁ±ªÂà´ËÆ°ÁÆóNLLÔºå
- Á¥ØÂä†ÂêéÂèñÂπ≥Âùá   


#### Generate next tokens

refine the model   
```python
class BigramLanguageModel(nn.Module):

	def __init__(self, vocab_size):
		super().__init__()
		self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

	def forward(self, idx, targets = None):
		logits = self.token_embedding_table(idx) # B, T, C
		
		if targets in None:
			loss = None
		else:
			B, T, C = logits.shape
			logits = logits.view(B*T, C)
			targets = targets.view(B*T)
			loss = F.cross_entropy(logits, targets)  # use nn.functional 
			
		return logits, loss

	def generate(self, idx, max_new_tokens):
		# idx here is (B,T) array of indices in the current context
		for _ in range(max_new_tokens):
			logits, loss = self(idx) # logits here will be a (B, T, C) array since no target provided
			logits = logits[:, -1, :] # only care about the final token (B, C)
			probs = F.softmax(logits, dim=-1) # (B, C)
			idx_next = torch.multinomial(probs, num_samples=1) # sample next token (B, 1)
			idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
		return idx
```

- We introduce `generate` function to generate `next_max_tokens`   
- `self(x)` call the forward function and return the logits(ignoring the loss)
- `torch.multinomial` is a function that sample idx according to `weight`(probs)  

**Ê≥®ÊÑèËøôÈáåÊòØÊåâbatchÁîüÊàêÁöÑ`next token`**   

```python
idx = torch.zeros((1,1), dtype=torch.long)
decode(model.generate(idx, 100)[0].tolist())
```

#### Train the model

```python
optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)
```

Karpathy said `AdamW` works well in setting that the learning rate is 3e-4.  Since the model is small, use 1e-3 instead(set much higher)    

```python
batch_size = 32
for step in range(100):
	xb, yb = get_batch("train")
	logits, loss = m(xb, yb)
	loss.backward()
	optimizer.step()
	print(loss.item())

print(loss.item())
```

> In the video, maybe 10000 steps works

#### `time.sleep(5)` 

Âà∞ËøôÈáåÊòØ‰∏Ä‰∫õÁÆÄÂçïÁöÑÂÆûÁé∞   
Â∑≤ÁªèÂÆûÁé∞ÁöÑÂÜÖÂÆπÂ¶Ç‰∏ãÔºö  
- ‰∏Ä‰∏™ÁÆÄÊòìÁöÑcharacter size tokenizer
- ‰∏Ä‰∏™ÊåâÈöèÊú∫Êï∞Ëé∑ÂæóbatchÁöÑdataloader
- ‰∏Ä‰∏™baseline model  Bigram language model

‰ΩÜÊòØÂ≠òÂú®Âá†‰∏™ÈóÆÈ¢ò  
- È¶ñÂÖàbatchÊòØÈöèÊú∫ÈÄâÂèñÁöÑÔºåÂπ∂‰∏çÈÅçÂéÜÊï¥‰∏™Êï∞ÊçÆÈõÜÔºåËøôÂØºËá¥‰∫ÜÂæàÂ§ßÁöÑÈöèÊú∫ÊÄß(noise)
	- ÂáèÂ∞ëËøô‰∫õnoise, KarpathyÂú®Â§ñÈù¢Â•ó‰∏ä‰∏ÄÂ±ÇÂæ™ÁéØÔºåÂç≥Á¥ØËÆ°‰∏ÄÂÆöÊ¨°Êï∞ÁöÑbatchÔºåÂΩìÂæ™ÁéØÁªìÊùüÂêéËÆ°ÁÆóÂπ≥ÂùálossÔºåÂ∞ÜÊ≠§‰Ωú‰∏∫‰∏ÄÊ¨°iteration  üëà ÊúâÊó†ÂèØËÉΩËá™Â∑±ÂÜô‰∏Ä‰∏™ÁÆÄÂçïÁöÑdataloader
- Áé∞Âú®ÁöÑËÆ°ÁÆóÊòØÂú®CPU‰∏äÔºåÈúÄË¶ÅËΩ¨Êç¢Âà∞GPU

> Âá∫Áé∞‰∫Ü‰∏Ä‰∏™`@torch.no_grad()` ÊåáÁ§∫ÂΩìÂâçÂáΩÊï∞‰∏çÈúÄË¶ÅÊ¢ØÂ∫¶(torch‰∏çÈúÄË¶ÅÁª¥Êä§Ê¢ØÂ∫¶Áä∂ÊÄÅ)ÔºåÂä†ÈÄüËÆ°ÁÆóÁöÑÊäÄÂ∑ß   


### Self-attention block

#### Mathematical trick in self-attention

We need the current token to conmunicate with the previous tokens   
The simplest way is average   

For intuitive, a simple program can be writen as   
```python
x = torch.rand
xbow = torch.zeros((B, T, C))
for b in range(B):
	for t in range(T):
		xprev = x[b, :t+1] # (t, C)
		xbow[b, t] = torch.mean(xprev, 0)
```

> `xbow` means bag of words   

`xbow` contains the mean of previous tokens(include the current one) at every position  

But the implementation is not efficient enough, using matrix multiplication instead of traversing in two loop    

È¶ñÂÖàÔºåÁü©ÈòµÁõ∏‰πò$A \times B$ Êúâ‰∏§‰∏™ËßíÂ∫¶Ôºå‰∏Ä‰∏™ÊòØÁªìÊûú$C$Á¨¨$i$Ë°åÊòØBÊØè‰∏ÄË°å‰æùÊçÆAÁöÑÁ¨¨$i$Ë°åÂä†ÊùÉÊ±ÇÂíåÁöÑÁªìÊûú(Ë°åÁöÑËßíÂ∫¶)Ôºå‰∏Ä‰∏™ÊòØCÁöÑÁ¨¨$i$ÂàóÔºåÊòØAÁöÑÊØè‰∏ÄÂàó‰æùÊçÆBÁöÑÁ¨¨$i$ÂàóÂä†ÊùÉÊ±ÇÂíåÁöÑÁªìÊûú   

‰ªéË°åÁöÑËßíÂ∫¶Êù•ËØ¥  
$$
c_{ij} = \mathbf{a}_i \cdot \mathbf{b}_j
$$
Âç≥aÁöÑÁ¨¨iË°å‰∏éaÁöÑÁ¨¨jÂàóÁöÑÁÇπ‰πò   

Âõ†Ê≠§ÔºåÂà©Áî®ËøôÊ†∑ÁöÑÁâπÊÄßÔºåÂèØ‰ª•Áõ¥Êé•ÂØπtokenËøõË°åÁ¥ØÂä†ÂíåÂπ≥Âùá   

ËæìÂÖ•ÁöÑtokenÔºå‰∏Ä‰∏™ÊâπÊ¨°‰∏∫Áü©Èòµ B $(T, C)$ÔºåÊàë‰ª¨ÈúÄË¶ÅÊó∂Èó¥Áª¥Â∫¶‰∏äÁöÑÂùáÂÄºÔºåÂç≥Âú®Ë°åËßíÂ∫¶Á¥ØÂä†Âπ∂Ê±ÇÂπ≥Âùá„ÄÇÂè™ÈúÄË¶ÅÂ∞ÜAËÆæÁΩÆ‰∏∫1ÁöÑ‰∏ã‰∏âËßíÁü©ÈòµÔºåÊàë‰ª¨Â∞±ËÉΩÂæóÂà∞Áü©ÈòµBË°å‰∏äÁöÑÁ¥ØÂä†ÁªìÊûú   
ÂºïÂÖ•ÂΩí‰∏ÄÂåñÔºåÂàôÂè™ÈúÄË¶ÅËÆ©‰∏ã‰∏âËßíÁü©ÈòµÁöÑÊØè‰∏ÄË°åÈô§‰ª•Ëøô‰∏ÄË°åÁöÑÂíå(Âç≥Ë°åÊï∞)   

```python
wei = torch.tril(torch.ones(T, T)) # 1 ÁöÑ‰∏ã‰∏âËßíÈòµ
wei = wei / wei.sum(1, keepdim=True) # ÂàóËßíÂ∫¶Âä†ÂíåÂπ∂‰øùÊåÅÁª¥Êï∞
xbow = wei @ x # (T, T) @ (B, T, C) broadcastË°•ÂÖ®Batch_size, (B,T,T) @ (B,T,C) --> (B,T,C)
```

> ÂèØ‰ª•Áî® `torch.allclose(xbow, xbow2)` ÊØîËæÉ‰∏§ÁßçÊñπÊ≥ïÁªìÊûúÊòØÂê¶‰∏ÄËá¥


A more readable version   
```python
tril = torch.tril(torch.ones(T, T))
wei = torch.zeros((T, T))
wei = wei.masked_fill(tril == 0, float('-inf'))
wei = F.softmax(wei, dim=-1)
xbow3 = wei @ x
```
ËøôÊÆµ‰ª£Á†ÅÁöÑÊÄùË∑ØÊúçÂä°‰∫éself-attention  
- È¶ñÂÖàÂæóÂà∞‰∏ã‰∏âËßíÁü©ÈòµÔºåË°®Êòé‰∏çËÄÉËôëÊú™Êù•‰ø°ÊÅØ
- ÁÑ∂ÂêéÂæóÂà∞ÊùÉÈáçÁü©ÈòµÔºåÂàùÂßãÂåñ‰∏∫0Ôºå‰ΩÜÂÆûÈôÖ‰∏äÂΩìÊ≥®ÊÑèÂäõË¢´ËÆ°ÁÆóÂêé‰ºöÊòØÂÖ∂‰ªñÊï∞ÂÄºÔºåÂ∞ÜÊú™Êù•‰ø°ÊÅØÊé©ÁõñÊàê`-inf`  
- ÁªèËøásoftmaxÂæóÂà∞ÊúÄÁªàÊùÉÈáç

#### Self-attention block   

Firstly we need to modify the bigram language model by adding linear head and positional embedding   

```python
class BigramLanguageModel(nn.Module):

	def __init__(self, vocab_size):
		super().__init__()
		self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
		self.postional_embedding_table = nn.Embedding(block_size, n_embd)
		self.lm_head = nn.linear(n_embd, vocab_size)

	def forward(self, idx, targets = None):
		B, T = idx.shape
		token_embd = self.token_embedding_table(idx) # B, T, C
		pos_embd = self.positonal_embedding_table(torch.arange(T, device=device)) # T, C
		x = token_embd + pos_embd  # broadcast
		logits = self.lm_head(x)

		if targets in None:
			loss = None
		else:
			B, T, C = logits.shape
			logits = logits.view(B*T, C)
			targets = targets.view(B*T)
			loss = F.cross_entropy(logits, targets)  # use nn.functional 
			
		return logits, loss

	def generate(self, idx, max_new_tokens):
		# idx here is (B,T) array of indices in the current context
		for _ in range(max_new_tokens):
			logits, loss = self(idx) # logits here will be a (B, T, C) array since no target provided
			logits = logits[:, -1, :] # only care about the final token (B, C)
			probs = F.softmax(logits, dim=-1) # (B, C)
			idx_next = torch.multinomial(probs, num_samples=1) # sample next token (B, 1)
			idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
		return idx
```


And then we implement the self-attention block   
```python
head_size = 16
query = torch.nn.linear(n_embd, head_size, bias = False)
key = torch.nn.linear(n_embd, head_size, bias = False)
value = torch.nn.linear(n_embd, head_size, bias = False) # bias = False ‰Ωølinear head Âè™Áî®‰∫éÊîπÂèòÁª¥Â∫¶
# Q K V
Q = query(x) # B, T, head_size
K = key(x)  # B, T, head_size
V = value(x) # B, T, head_size
# attention matrix
wei = Q @ K.tranpose(-2, -1)  # B, T, T
# masked
tril = torch.tril(torch.ones(T, T))
wei = wei.masked_fill(tril == 0, float('-inf'))
wei = F.softmax(wei, dim=-1)  # B, T, T
# final output
out = wei @ V
```
- Â¶ÇÊûúÊòØencoder blockÔºåÂàôÊ≤°Êúâ`tril`ÁöÑÈÉ®ÂàÜ(‰∏çÈúÄË¶Åmask)
- Ê≠§Êó∂ÁöÑattentionÊ≤°ÊúâÈô§‰ª• $\sqrt{d_k}$ (`head_size`)/Ëøò‰∏çÊòØscaled self-attention

##### ‰∏∫‰ªÄ‰πàË¶ÅScaled

ÂáèÂ∞èÊñπÂ∑Æ   
softmaxÊòØ‰∏Ä‰∏™ÂΩí‰∏ÄÂåñÂáΩÊï∞ÔºåÂ¶ÇÊûúËøõË°åÂΩí‰∏ÄÁöÑÊï∞ÊçÆÂ∑ÆË∑ùËøáÂ§ßÔºåÂÆπÊòìÂØºËá¥ÂáΩÊï∞Êî∂ÊïõÂà∞ÊûÅÁ´Ø(1)  
Âç≥ÔºåÊ≥®ÊÑèÂäõ/token‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄßÂè™Âú®‰∏™Âà´Êï∞ÂÄºÂ§ßÁöÑpositionÊúâÊïà„ÄÇ      
ÈÄöËøáscaledÔºåÈô§‰ª•$\sqrt{d_k}$Êù•ÂáèÂ∞èÊñπÂ∑Æ     
[[Why dividing by square root of dimension of key vector]]   

##### Attention Head

```python
class Head(nn.Module):

	def __init__(self, head_size):
		super().__init__()
		self.key = nn.Linear(n_embd, head_size, bias = False)
		self.query = nn.Linear(n_embd, head_size, bias = False)
		self.value = nn.Linear(n_embd, head_size, bias = False)
		self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))

	def forward(self, x):
		B, T, C = x.shape
		k = self.key(x)  # (B, T, head_size)
		q = self.query(x) # (B, T, head_size)
		v = self.value(x) # (B, T, head_size)
		# compute attention score ("affinities")
		wei = q @ k.transpose(-2, -1) * C**(-0.5) # (B, T, T)
		wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
		wei = F.softmax(wei, dim=-1) # (B, T, T)

		v = value(x)
		out = wei @ v # (B, T, head_size)
		return out
```
- triangular module is not a parameter module of the model. In pytorch, it should be assign with `register_buffer`   

```python
class MultiHeadAttention(nn.Module):

	def __init__(self, num_heads, head_size):
		super().__init__()
		self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])

	def forward(self, x):
		return torch.cat([h(x) for h in self.heads], dim = -1)
```
- concatenate over channel dimension

##### FeedForward layer

```python
class FeedForward(nn.Module):
	def __init__(self, n_embd):
		super().__init__()
		self.net = nn.Sequential(
			nn.Linear(n_embd, n_embd),
			nn.ReLU(),
		)
```

Add computation layers   
ÂèØ‰ª•ËÆ§‰∏∫ÊòØÂ∏ÆÂä©Ëé∑ÂæóÊ≥®ÊÑèÂäõ‰πãÂêéÁöÑtokenÂéªÊÄùËÄÉÊ≥®ÊÑèÂäõÁöÑÂΩ±Âìç    
> ËøôÈáåÁöÑÂÆûÁé∞Â∞ÜbatchÂíåTimeÈÉΩÂΩìÊàêÊòØBatchÔºåÂØπÊØè‰∏Ä‰∏™tokenÈÉΩËøõË°åËÆ°ÁÆó    

##### Attention block

![[Pasted image 20250117165015.png]]    


```python
class Block(nn.Module):
	def __init__(self, n_embd, n_head):
		super()._init__()
		head_size = n_embd // n_head
		self.mha = MultiHeadAttention(n_head, head_size)
		self.ffwd = FeedForward(n_embd)
	def forward(self, x):
		x = self.mha(x)
		x = self.ffwd(x)
		return x
```


```python

class BigramLanguageModel(nn.Module):

	def __init__(self, vocab_size):
		super().__init__()
		self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
		self.postional_embedding_table = nn.Embedding(block_size, n_embd)
		self.blocks = nn.Sequential(
			Block(n_embd, n_head=4),
			Block(n_embd, n_head=4),
			Block(n_embd, n_head=4),
		)
		self.lm_head = nn.linear(n_embd, vocab_size)
		

	def forward(self, idx, targets = None):
		B, T = idx.shape
		token_embd = self.token_embedding_table(idx) # B, T, C
		pos_embd = self.positonal_embedding_table(torch.arange(T, device=device)) # T, C
		x = token_embd + pos_embd  # broadcast
		x = self.blocks(x)
		logits = self.lm_head(x)
		
		if targets in None:
			loss = None
		else:
			B, T, C = logits.shape
			logits = logits.view(B*T, C)
			targets = targets.view(B*T)
			loss = F.cross_entropy(logits, targets)  # use nn.functional 
			
		return logits, loss

	def generate(self, idx, max_new_tokens):
		# idx here is (B,T) array of indices in the current context
		for _ in range(max_new_tokens):
			logits, loss = self(idx) # logits here will be a (B, T, C) array since no target provided
			logits = logits[:, -1, :] # only care about the final token (B, C)
			probs = F.softmax(logits, dim=-1) # (B, C)
			idx_next = torch.multinomial(probs, num_samples=1) # sample next token (B, 1)
			idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
		return idx
```

#### Tricks borrowed from the original paper

##### Residual connections

add resudual connection and some projections   
```python
class Block(nn.Module):
	def __init__(self, n_embd, n_head):
		super()._init__()
		head_size = n_embd // n_head
		self.mha = MultiHeadAttention(n_head, head_size)
		self.ffwd = FeedForward(n_embd)
	def forward(self, x):
		x = x + self.mha(x)
		x = x + self.ffwd(x)
		return x
```

```python
class MultiHeadAttention(nn.Module):

	def __init__(self, num_heads, head_size):
		super().__init__()
		self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
		self.proj = nn.Linear(n_embd, n_embd)

	def forward(self, x):
		out = torch.cat([h(x) for h in self.heads], dim = -1)
		out = self.proj(out)
		return out
```

```python
class FeedForward(nn.Module):
	def __init__(self, n_embd):
		super().__init__()
		self.net = nn.Sequential(
			nn.Linear(n_embd, n_embd),
			nn.ReLU(),
			nn.Linear(n_embd, n_embd),
		)
```

And the original paper use 4 times embedding dimension 

```python
class FeedForward(nn.Module):
	def __init__(self, n_embd):
		super().__init__()
		self.net = nn.Sequential(
			nn.Linear(n_embd, 4 * n_embd),
			nn.ReLU(),
			nn.Linear(4 * n_embd, n_embd),
		)
```

#### Layer Norm & Drop out

Transformer add layer norm after the transformation, but for now it is common to apply layer norm before transformation  
We can add layer norm in   
- before transformation
- at the end of transformer

```python
class Block(nn.Module):
	def __init__(self, n_embd, n_head):
		super()._init__()
		head_size = n_embd // n_head
		self.mha = MultiHeadAttention(n_head, head_size)
		self.ffwd = FeedForward(n_embd)
		self.ln1 = nn.LayerNorm(n_embd)
		self.ln3 = nn.LayerNorm(n_embd)
	def forward(self, x):
		x = x + self.mha(ln1(x))
		x = x + self.ffwd(ln2(x))
		return x
```

```python

class BigramLanguageModel(nn.Module):

	def __init__(self, vocab_size):
		super().__init__()
		self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
		self.postional_embedding_table = nn.Embedding(block_size, n_embd)
		self.blocks = nn.Sequential(
			Block(n_embd, n_head=4),
			Block(n_embd, n_head=4),
			Block(n_embd, n_head=4),
			nn.LayerNorm(n_embd),
		)
		self.lm_head = nn.linear(n_embd, vocab_size)
```


Drop out can be added  
- right before the residual connection back
- after computing attention score(randomly prevent some of the nodes from communication)

```python
class MultiHeadAttention(nn.Module):

	def __init__(self, num_heads, head_size):
		super().__init__()
		self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
		self.proj = nn.Linear(n_embd, n_embd)
		self.dropout = nn.Dropout(dropout)
	def forward(self, x):
		out = torch.cat([h(x) for h in self.heads], dim = -1)
		out = self.proj(out)
		out = self.dropout(out)
		return out
```

```python
class FeedForward(nn.Module):
	def __init__(self, n_embd):
		super().__init__()
		self.net = nn.Sequential(
			nn.Linear(n_embd, 4 * n_embd),
			nn.ReLU(),
			nn.Linear(4 * n_embd, n_embd),
			nn.Dropout(dropout),
		)
```


```python
class Head(nn.Module):

	def __init__(self, head_size):
		super().__init__()
		self.key = nn.Linear(n_embd, head_size, bias = False)
		self.query = nn.Linear(n_embd, head_size, bias = False)
		self.value = nn.Linear(n_embd, head_size, bias = False)
		sefl.dropour = nn.Dropout(dropout)
		self.register_buffer('tril', torch.tril(torch.ones(block_size, block_szie)))

	def forward(self, x):
		B, T, C = x.shape
		k = self.key(x)  # (B, T, head_size)
		q = self.query(x) # (B, T, head_size)
		v = self.value(x) # (B, T, head_size)
		# compute attention score ("affinities")
		wei = q @ k.transpose(-2, -1) * C**(-0.5) # (B, T, T)
		wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
		wei = F.softmax(wei, dim=-1) # (B, T, T)
		wei = self.dropout(wei)
		v = value(x)
		out = wei @ v # (B, T, head_size)
		return out
```


#### Then scale up

```python

class BigramLanguageModel(nn.Module):

	def __init__(self, vocab_size):
		super().__init__()
		self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
		self.postional_embedding_table = nn.Embedding(block_size, n_embd)
		self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])
		self.ln_f = nn.LayerNorm(n_embd)
		self.lm_head = nn.linear(n_embd, vocab_size)
		
	def forward(self, idx, targets = None):
		B, T = idx.shape
		token_embd = self.token_embedding_table(idx) # B, T, C
		pos_embd = self.positonal_embedding_table(torch.arange(T, device=device)) # T, C
		x = token_embd + pos_embd  # broadcast
		x = self.blocks(x)
		x = self.ln_f(x)
		logits = self.lm_head(x)
		
		if targets in None:
			loss = None
		else:
			B, T, C = logits.shape
			logits = logits.view(B*T, C)
			targets = targets.view(B*T)
			loss = F.cross_entropy(logits, targets)  # use nn.functional 
			
		return logits, loss

	def generate(self, idx, max_new_tokens):
		# idx here is (B,T) array of indices in the current context
		for _ in range(max_new_tokens):
			logits, loss = self(idx) # logits here will be a (B, T, C) array since no target provided
			logits = logits[:, -1, :] # only care about the final token (B, C)
			probs = F.softmax(logits, dim=-1) # (B, C)
			idx_next = torch.multinomial(probs, num_samples=1) # sample next token (B, 1)
			idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
		return idx
```

adjust batch_size, block_size, iteration times, n_embd, n_head, n_layer according to compute resources    

